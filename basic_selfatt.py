# -*- coding: utf-8 -*-
"""basic_selfAtt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pe_O7Aa01BUHTDprjUHZdTLWexWk5Mxv
"""

import numpy as np
import netket as nk

import jax
import jax.numpy as jnp

import flax
import flax.linen as nn

class MultiHead_Att(nn.Module):
  ### Parametros
  layers: int #capas
  heads: int # cabezas de attention
  dk: int #dimension matrices entrenables

  ###call
  @nn.compact
  def __call__(self, x):
    dimension = x.shape[-1]
    dimension_head = self.heads * self.dk
    w_shape = (dimension, dimension, self.heads)

    #definimos matrices de entrenamiento
    for i in range(self.layers):

      #-----------------------------------------------------------------------------------------------#

      weight_Q = self.param(f'weight_Q_head{i}', nn.initializers.xavier_uniform(), w_shape)
      weight_K = self.param(f'weight_K_head{i}', nn.initializers.xavier_uniform(), w_shape)
      weight_V = self.param(f'weight_V_head{i}', nn.initializers.xavier_uniform(), w_shape)

      # Bloque Self_Attention
      Q = jnp.einsum('bd,ddh->bhd', x, weight_Q)
      K = jnp.einsum('bd,ddh->bhd', x, weight_K)
      V = jnp.einsum('bd,ddh->bhd', x, weight_V)


      ##DENTRO DE CADA CABEZA##
      logits = jnp.einsum('bhd,bhd->bh', Q, K) / jnp.sqrt(dimension)
      weights = jax.nn.softmax(logits, axis=-1)


      att_output = jnp.einsum('bh,bhd->bhd', weights, V)

      att_output = jnp.mean(att_output, axis=1)


      #-----------------------------------------------------------------------------------------------#
      #ESTAMOS EN UN BUCLE IMPORTANTE PONER NOMBRES
      ##Residuo2##
      x = x + att_output
      x = nn.LayerNorm(name=f'ln1_{i}')(x)

      ##MLP##
      mlp = nn.Dense(dimension * 4, name=f'mlp_up_{i}')(x)
      mlp = nn.gelu(mlp)
      mlp = nn.Dense(dimension, name=f'mlp_down_{i}')(mlp)

      ##Residuo2##
      x = x + mlp
      x = nn.LayerNorm(name=f'ln2_{i}')(x)


    output = nn.Dense(1)(x) # log amplitud final
    output = output.squeeze(-1) # Squeeze the last dimension

    return output

